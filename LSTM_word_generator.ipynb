{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp_Lq-5l9ndl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 1: Text To Supervised Datset"
      ],
      "metadata": {
        "id": "ZPT2VKaoKLwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load .txt file"
      ],
      "metadata": {
        "id": "LKeBWu-D_pVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/text generator dataset.txt', 'r') as f:\n",
        "  text = f.read().lower()"
      ],
      "metadata": {
        "id": "4ahktJLc_tWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split text into sentences using '.'\n",
        "sentences = text.split('.')"
      ],
      "metadata": {
        "id": "3cp681rSEQ2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove empty sentences and strip extra spaces\n",
        "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]"
      ],
      "metadata": {
        "id": "7wQEihYAEY6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total sentences:\", len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHpNJcomEcDA",
        "outputId": "ea6ea312-5a39-47f7-e94b-6aa598f62817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the words"
      ],
      "metadata": {
        "id": "PWncYLkEBRyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvlH4oeDBRXp",
        "outputId": "30e45789-3d6b-47da-cd03-3afeff60746a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download(\"punkt_tab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dAdfwjJAlL4",
        "outputId": "4086c7ad-1d3e-4f3f-f622-3f2feccdab80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize each sentence separately\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "Ka0oDlg1BbE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenized_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUKhUOLGBmUy",
        "outputId": "4b55d41a-9a1c-47ff-f38c-aee86c784321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First tokenized sentence:\", tokenized_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzshphEOEpl0",
        "outputId": "5e2efd07-ffd3-43a8-8741-aacb2f964952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First tokenized sentence: ['the', 'sun', 'was', 'shining', 'brightly', 'in', 'the', 'clear', 'blue', 'sky', ',', 'and', 'a', 'gentle', 'breeze', 'rustled', 'the', 'leaves', 'of', 'the', 'tall', 'trees']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten the list of tokenized sentences to get all words\n",
        "all_tokens = [word for sentence in tokenized_sentences for word in sentence]"
      ],
      "metadata": {
        "id": "YSuNbRc7DDmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocabulary\n",
        "word_counts = Counter(all_tokens)\n",
        "vocab = {word: idx for idx, (word, _) in enumerate(word_counts.items(), start=1)}"
      ],
      "metadata": {
        "id": "X_aqxPsvE_fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a special token for unknown words\n",
        "vocab[\"<UNK>\"] = len(vocab) + 1"
      ],
      "metadata": {
        "id": "krF2I5ObDJVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each sentence to its corresponding indexes\n",
        "indexed_sentences = [[vocab.get(word, vocab[\"<UNK>\"]) for word in sentence] for sentence in tokenized_sentences]"
      ],
      "metadata": {
        "id": "C8MOmm1yDNLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First indexed sentence:\", indexed_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8vNdiBdBxSw",
        "outputId": "9ec8e9dc-ff6a-46a5-d399-6bf69348f45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First indexed sentence: [1, 2, 3, 4, 5, 6, 1, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 16, 17, 1, 18, 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Supervised Dataset"
      ],
      "metadata": {
        "id": "MCh1Zxc3F8rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(indexed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WyIa86xCYPd",
        "outputId": "d53e1fe9-d597-4453-a58b-02c1afdcec9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "268"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence = []\n",
        "target_sequence = []\n",
        "\n",
        "for sentence in indexed_sentences:\n",
        "  for i in range(1, len(sentence)):\n",
        "    context = sentence[:i]\n",
        "    target = sentence[i]\n",
        "    input_sequence.append(context)\n",
        "    target_sequence.append(target)\n"
      ],
      "metadata": {
        "id": "931clZJ7D113"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(target_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDEty-JDGlcr",
        "outputId": "8eb1f2f4-6df5-4c02-a172-cb204d878746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4883"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find out the maximum length in sentence\n",
        "max_length = max(len(seq) for seq in input_sequence)\n",
        "\n",
        "print(max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D0nykwKHR_i",
        "outputId": "192cde0f-6f29-44b2-e599-634145a2fefb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero Padding using PyTorch"
      ],
      "metadata": {
        "id": "4Ls7GjgLIYUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert into pytorch tensors\n",
        "tensor_sentences = [torch.tensor(sentence) for sentence in input_sequence]"
      ],
      "metadata": {
        "id": "E3mjQOmXHnke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "padded_sequences = pad_sequence(tensor_sentences, batch_first=True, padding_value=0)"
      ],
      "metadata": {
        "id": "EfsTSPSMIpBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Padded sequences shape:\", padded_sequences.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jJn3Q2xJavJ",
        "outputId": "ab6853ba-657d-464d-8ac1-c82b2d46b37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded sequences shape: torch.Size([4883, 43])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(padded_sequences[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_OpbN_lI6Tm",
        "outputId": "61ea11b5-50b1-4e7f-d2b2-179f4e064dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Hot Encoding to `taget_sequence`"
      ],
      "metadata": {
        "id": "6KTikK9nK5Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to tensor\n",
        "target_tensor = torch.tensor(target_sequence)"
      ],
      "metadata": {
        "id": "eUYyQ_AkI9OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47QnmIqFKUjR",
        "outputId": "66133beb-a3a5-46af-a76e-3babe33a2f72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4883])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# length of vacabulory (add 1 to cover the last word as onehotencoding starts from 0 & vocab starts from 1)\n",
        "vocab_size = len(vocab) + 1\n",
        "\n",
        "# Convert tensor to one-hot encoding\n",
        "one_hot_targets = F.one_hot(target_tensor, num_classes=vocab_size).float()"
      ],
      "metadata": {
        "id": "ZcLcDGAkLKTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_targets[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7HvyajHL0Ju",
        "outputId": "8a326fc7-b6e3-45d8-f12b-1d9d1d1aedfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 1.,  ..., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_targets.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WvgsUdkTt1w",
        "outputId": "cfb7632d-e4c3-495c-91d6-95e7e44a21f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4883, 1613])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_sequences\n",
        "y = one_hot_targets"
      ],
      "metadata": {
        "id": "xjeJ_7iFf3JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAieBXaIf63n",
        "outputId": "3763dd48-5399-48d0-d767-9beaafcb43c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4883, 43])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WQ_gBUZf8uU",
        "outputId": "f3dd406c-3552-4557-f0dd-9dac3082b11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4883, 1613])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 2: Model Architecture"
      ],
      "metadata": {
        "id": "fSusPG7TKWpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3 layer architecture** <br>\n",
        "1 -> embedding <br>\n",
        "2 -> LSTM <br>\n",
        "3 -> Dense <br>\n",
        "\n"
      ],
      "metadata": {
        "id": "BOoHoX9sM3ng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create model"
      ],
      "metadata": {
        "id": "bve714RCPzpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMNextWordPredictor(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "    super(LSTMNextWordPredictor, self).__init__()\n",
        "\n",
        "    # 1. Embedding Laye\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # 2. LSTM Layer\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "\n",
        "    # 3. Dense Layer\n",
        "    self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "\n",
        "    lstm_out, _ = self.lstm(embedded)\n",
        "\n",
        "    output = self.fc(lstm_out[:, -1, :])  # Output of the last time step\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "YAdO3RerL2Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Hyperparameters"
      ],
      "metadata": {
        "id": "OpdLS99IP4bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "vocab_size = len(vocab) + 1  # Add 1 for padding/unknown\n",
        "embedding_dim = 100  # Size of word embeddings\n",
        "hidden_dim = 150  # LSTM hidden state size\n",
        "num_layers = 3  # Number of LSTM layers\n",
        "num_epochs = 100 # Number of epochs\n",
        "learning_rate = 0.0005"
      ],
      "metadata": {
        "id": "h7dQqwScOxG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model\n",
        "model = LSTMNextWordPredictor(vocab_size,\n",
        "                              embedding_dim,\n",
        "                              hidden_dim,\n",
        "                              num_layers)"
      ],
      "metadata": {
        "id": "gjqEKQUWPTqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model summary\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnmvWm-2Peow",
        "outputId": "654c06d2-84cc-4ee4-9295-d0ebe62fe5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMNextWordPredictor(\n",
            "  (embedding): Embedding(1613, 100)\n",
            "  (lstm): LSTM(100, 150, num_layers=3, batch_first=True)\n",
            "  (fc): Linear(in_features=150, out_features=1613, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Loss & Optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "uuGcC_yxPfFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "QzBrUT8aP8Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYA62qwyV_eo",
        "outputId": "6e8b5055-bc14-442b-ed88-06056bccf111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4u51jmwWA7x",
        "outputId": "bc98002e-79c1-4c80-a857-0648082aee22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMNextWordPredictor(\n",
              "  (embedding): Embedding(1613, 100)\n",
              "  (lstm): LSTM(100, 150, num_layers=3, batch_first=True)\n",
              "  (fc): Linear(in_features=150, out_features=1613, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(output, y):\n",
        "    \"\"\"\n",
        "    Computes accuracy by comparing model predictions with actual labels.\n",
        "\n",
        "    Args:\n",
        "        output (torch.Tensor): Raw model output logits of shape (batch_size, vocab_size)\n",
        "        y (torch.Tensor): True class indices of shape (batch_size)\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy percentage\n",
        "    \"\"\"\n",
        "    # Get the predicted class (highest probability in vocab_size dimension)\n",
        "    predictions = torch.argmax(output, dim=1)  # Shape: [batch_size]\n",
        "\n",
        "    # Ensure y is in index format, not one-hot\n",
        "    if y.dim() > 1:\n",
        "        y = torch.argmax(y, dim=1)\n",
        "\n",
        "    # Compute the number of correct predictions\n",
        "    correct = (predictions == y).sum().item()\n",
        "\n",
        "    # Compute accuracy percentage\n",
        "    accuracy = (correct / y.size(0)) * 100\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "quhVcLMFzW--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    x = X.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "    output = model(x)  # Forward pass\n",
        "\n",
        "    loss = criterion(output, y.argmax(dim=1)) # Compute loss\n",
        "    loss.backward()  # Backpropagation\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # Gradient clipping\n",
        "\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = calculate_accuracy(output, y.argmax(dim=1))\n",
        "\n",
        "    # if epoch % 100 == 0:\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "nbFR1hwTme92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e703a71e-f18b-4591-f8ce-665ce021da16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 7.3902, Accuracy: 0.0205\n",
            "Epoch [2/100], Loss: 7.3804, Accuracy: 0.0410\n",
            "Epoch [3/100], Loss: 7.3702, Accuracy: 7.2701\n",
            "Epoch [4/100], Loss: 7.3582, Accuracy: 7.2701\n",
            "Epoch [5/100], Loss: 7.3431, Accuracy: 7.2701\n",
            "Epoch [6/100], Loss: 7.3231, Accuracy: 7.2701\n",
            "Epoch [7/100], Loss: 7.2958, Accuracy: 7.2701\n",
            "Epoch [8/100], Loss: 7.2584, Accuracy: 7.2701\n",
            "Epoch [9/100], Loss: 7.2083, Accuracy: 7.2701\n",
            "Epoch [10/100], Loss: 7.1441, Accuracy: 7.2701\n",
            "Epoch [11/100], Loss: 7.0672, Accuracy: 7.2701\n",
            "Epoch [12/100], Loss: 6.9817, Accuracy: 7.2701\n",
            "Epoch [13/100], Loss: 6.8935, Accuracy: 7.2701\n",
            "Epoch [14/100], Loss: 6.8079, Accuracy: 7.2701\n",
            "Epoch [15/100], Loss: 6.7277, Accuracy: 7.2701\n",
            "Epoch [16/100], Loss: 6.6534, Accuracy: 7.2701\n",
            "Epoch [17/100], Loss: 6.5839, Accuracy: 7.2701\n",
            "Epoch [18/100], Loss: 6.5180, Accuracy: 7.2701\n",
            "Epoch [19/100], Loss: 6.4551, Accuracy: 7.2701\n",
            "Epoch [20/100], Loss: 6.3953, Accuracy: 7.2701\n",
            "Epoch [21/100], Loss: 6.3389, Accuracy: 7.2701\n",
            "Epoch [22/100], Loss: 6.2863, Accuracy: 7.2701\n",
            "Epoch [23/100], Loss: 6.2381, Accuracy: 7.2701\n",
            "Epoch [24/100], Loss: 6.1948, Accuracy: 7.2701\n",
            "Epoch [25/100], Loss: 6.1567, Accuracy: 7.2701\n",
            "Epoch [26/100], Loss: 6.1241, Accuracy: 7.2701\n",
            "Epoch [27/100], Loss: 6.0971, Accuracy: 7.2701\n",
            "Epoch [28/100], Loss: 6.0755, Accuracy: 7.2701\n",
            "Epoch [29/100], Loss: 6.0589, Accuracy: 7.2701\n",
            "Epoch [30/100], Loss: 6.0466, Accuracy: 7.2701\n",
            "Epoch [31/100], Loss: 6.0377, Accuracy: 7.2701\n",
            "Epoch [32/100], Loss: 6.0315, Accuracy: 7.2701\n",
            "Epoch [33/100], Loss: 6.0272, Accuracy: 7.2701\n",
            "Epoch [34/100], Loss: 6.0240, Accuracy: 7.2701\n",
            "Epoch [35/100], Loss: 6.0215, Accuracy: 7.2701\n",
            "Epoch [36/100], Loss: 6.0193, Accuracy: 7.2701\n",
            "Epoch [37/100], Loss: 6.0172, Accuracy: 7.2701\n",
            "Epoch [38/100], Loss: 6.0151, Accuracy: 7.2701\n",
            "Epoch [39/100], Loss: 6.0129, Accuracy: 7.2701\n",
            "Epoch [40/100], Loss: 6.0106, Accuracy: 6.4919\n",
            "Epoch [41/100], Loss: 6.0082, Accuracy: 6.4919\n",
            "Epoch [42/100], Loss: 6.0058, Accuracy: 6.4919\n",
            "Epoch [43/100], Loss: 6.0034, Accuracy: 6.4919\n",
            "Epoch [44/100], Loss: 6.0011, Accuracy: 6.4919\n",
            "Epoch [45/100], Loss: 5.9989, Accuracy: 6.4919\n",
            "Epoch [46/100], Loss: 5.9969, Accuracy: 6.4919\n",
            "Epoch [47/100], Loss: 5.9952, Accuracy: 6.4919\n",
            "Epoch [48/100], Loss: 5.9936, Accuracy: 6.4919\n",
            "Epoch [49/100], Loss: 5.9923, Accuracy: 6.4919\n",
            "Epoch [50/100], Loss: 5.9912, Accuracy: 6.4919\n",
            "Epoch [51/100], Loss: 5.9903, Accuracy: 6.4919\n",
            "Epoch [52/100], Loss: 5.9895, Accuracy: 6.4919\n",
            "Epoch [53/100], Loss: 5.9889, Accuracy: 6.4919\n",
            "Epoch [54/100], Loss: 5.9884, Accuracy: 6.4919\n",
            "Epoch [55/100], Loss: 5.9880, Accuracy: 6.4919\n",
            "Epoch [56/100], Loss: 5.9877, Accuracy: 7.2701\n",
            "Epoch [57/100], Loss: 5.9873, Accuracy: 7.2701\n",
            "Epoch [58/100], Loss: 5.9870, Accuracy: 7.2701\n",
            "Epoch [59/100], Loss: 5.9867, Accuracy: 7.2701\n",
            "Epoch [60/100], Loss: 5.9864, Accuracy: 7.2701\n",
            "Epoch [61/100], Loss: 5.9861, Accuracy: 7.2701\n",
            "Epoch [62/100], Loss: 5.9858, Accuracy: 7.2701\n",
            "Epoch [63/100], Loss: 5.9855, Accuracy: 7.2701\n",
            "Epoch [64/100], Loss: 5.9853, Accuracy: 7.2701\n",
            "Epoch [65/100], Loss: 5.9850, Accuracy: 7.2701\n",
            "Epoch [66/100], Loss: 5.9848, Accuracy: 7.2701\n",
            "Epoch [67/100], Loss: 5.9846, Accuracy: 7.2701\n",
            "Epoch [68/100], Loss: 5.9843, Accuracy: 7.2701\n",
            "Epoch [69/100], Loss: 5.9842, Accuracy: 7.2701\n",
            "Epoch [70/100], Loss: 5.9840, Accuracy: 7.2701\n",
            "Epoch [71/100], Loss: 5.9838, Accuracy: 7.2701\n",
            "Epoch [72/100], Loss: 5.9837, Accuracy: 7.2701\n",
            "Epoch [73/100], Loss: 5.9835, Accuracy: 7.2701\n",
            "Epoch [74/100], Loss: 5.9834, Accuracy: 7.2701\n",
            "Epoch [75/100], Loss: 5.9833, Accuracy: 7.2701\n",
            "Epoch [76/100], Loss: 5.9832, Accuracy: 7.2701\n",
            "Epoch [77/100], Loss: 5.9831, Accuracy: 7.2701\n",
            "Epoch [78/100], Loss: 5.9831, Accuracy: 7.2701\n",
            "Epoch [79/100], Loss: 5.9830, Accuracy: 7.2701\n",
            "Epoch [80/100], Loss: 5.9830, Accuracy: 7.2701\n",
            "Epoch [81/100], Loss: 5.9829, Accuracy: 7.2701\n",
            "Epoch [82/100], Loss: 5.9829, Accuracy: 7.2701\n",
            "Epoch [83/100], Loss: 5.9829, Accuracy: 7.2701\n",
            "Epoch [84/100], Loss: 5.9829, Accuracy: 7.2701\n",
            "Epoch [85/100], Loss: 5.9828, Accuracy: 7.2701\n",
            "Epoch [86/100], Loss: 5.9828, Accuracy: 7.2701\n",
            "Epoch [87/100], Loss: 5.9828, Accuracy: 7.2701\n",
            "Epoch [88/100], Loss: 5.9828, Accuracy: 7.2701\n",
            "Epoch [89/100], Loss: 5.9827, Accuracy: 7.2701\n",
            "Epoch [90/100], Loss: 5.9827, Accuracy: 7.2701\n",
            "Epoch [91/100], Loss: 5.9827, Accuracy: 7.2701\n",
            "Epoch [92/100], Loss: 5.9826, Accuracy: 7.2701\n",
            "Epoch [93/100], Loss: 5.9826, Accuracy: 7.2701\n",
            "Epoch [94/100], Loss: 5.9826, Accuracy: 7.2701\n",
            "Epoch [95/100], Loss: 5.9825, Accuracy: 7.2701\n",
            "Epoch [96/100], Loss: 5.9825, Accuracy: 7.2701\n",
            "Epoch [97/100], Loss: 5.9825, Accuracy: 7.2701\n",
            "Epoch [98/100], Loss: 5.9825, Accuracy: 7.2701\n",
            "Epoch [99/100], Loss: 5.9825, Accuracy: 7.2701\n",
            "Epoch [100/100], Loss: 5.9824, Accuracy: 7.2701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 3: Predict the Next Word"
      ],
      "metadata": {
        "id": "T3kLRIuk0Vpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert the input text into numerical indices (tokenization)."
      ],
      "metadata": {
        "id": "x8AaVmHz0Zwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"As you ventured further\""
      ],
      "metadata": {
        "id": "HEu_otuMzgMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize = word_tokenize(text)\n",
        "\n",
        "tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUNr9-8t0hq2",
        "outputId": "1882f13e-1c9d-40eb-f0ad-8f3665f8de76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['As', 'you', 'ventured', 'further']"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexed_text = [vocab.get(word, vocab[\"<UNK>\"]) for word in tokenize]\n",
        "\n",
        "indexed_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZMzgLpJ05rc",
        "outputId": "defc6d82-af21-42b1-978c-00be08fa1b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1612, 68, 160, 161]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(model, word_tokenize, text, vocab, top_k=1):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  # Toeknize the text\n",
        "  tokenize = word_tokenize(text.lower())\n",
        "\n",
        "  # Write the index of the text\n",
        "  indexed_text = [vocab.get(word, vocab[\"<UNK>\"]) for word in tokenize]\n",
        "\n",
        "  # Convert to tensor and reshape to match input shape (batch_size=1, seq_length)\n",
        "  input_tensor = torch.tensor(indexed_text, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "  # Apply padding\n",
        "  # pad_length = max_length - input_tensor.shape[1]\n",
        "  # if pad_length > 0:\n",
        "  #     padding = torch.zeros((1, pad_length), dtype=torch.long).to(device)  # Padding with 0s\n",
        "  #     input_tensor = torch.cat((input_tensor, padding), dim=1)  # right padding\n",
        "  print(input_tensor.shape)\n",
        "\n",
        "  # Get model predictions\n",
        "  with torch.no_grad():  # No gradient computation needed\n",
        "      output = model(input_tensor)\n",
        "\n",
        "  # Get the predicted word index (last time step)\n",
        "  predicted_indices = torch.topk(output, top_k, dim=1).indices.squeeze(0)  # Get top-k predictions\n",
        "\n",
        "  # Convert index to word\n",
        "  index_to_word = {idx: word for word, idx in vocab.items()}  # Reverse tokenizer\n",
        "  predicted_words = [index_to_word[idx.item()] for idx in predicted_indices]\n",
        "\n",
        "  return predicted_words if top_k > 1 else predicted_words[0]"
      ],
      "metadata": {
        "id": "NG-5lX9n1cMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Your RNN\""
      ],
      "metadata": {
        "id": "OXC3aja23NKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_word = predict_next_word(model, word_tokenize, text, vocab, top_k=1)\n",
        "\n",
        "print(predicted_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cg_qvcR2pJB",
        "outputId": "3a9c01e0-d445-464c-d726-705860e4b573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2])\n",
            "the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BnyLpvum5ypY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}